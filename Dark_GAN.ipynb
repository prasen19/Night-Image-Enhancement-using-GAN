{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, ReLU, Input, concatenate, PReLU, Add\n",
    "from tensorflow.keras import Model\n",
    "#from IPython import display\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "#tf.keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "path_inp = 'F:/trial/'\n",
    "checkpoint_dir = 'D:/Prasen/Pix2Pix/Checkpoints_Gen2'\n",
    "reload_dir = 'D:/Prasen/Pix2Pix/IMP_Chechpoints/Exellents'\n",
    "res_dire_1 = 'D:/Prasen/Pix2Pix/Results/Gen_1/'\n",
    "res_dire_2 = 'D:/Prasen/Pix2Pix/Results/Gen_C/'\n",
    "\n",
    "def norm_train(inp):\n",
    "    image = tf.io.read_file(inp)    # Read any file and convert it into tensor\n",
    "    \n",
    "    image = tf.image.decode_jpeg(image)    # Convert tensor into uint8 type tensor such as image matrix\n",
    "\n",
    "    \n",
    "    w = tf.shape(image)[1]        # w is that coloumn value which will slpit a combined image into 2         \n",
    "\n",
    "    w = w // 2\n",
    "    \n",
    "    real_image = image[:, :w, :]       # separating 2 images by taking half of a combined image    \n",
    "    input_image = image[:, w:, :]\n",
    "\n",
    "    input_image = tf.cast(input_image, tf.float32)  # Converting uint8 to float32\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "    \n",
    "    input_image = (input_image / 255)\n",
    "    real_image = (real_image / 255)\n",
    "\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def norm_test(inp):\n",
    "    image = tf.io.read_file(inp)    # Read any file and convert it into tensor\n",
    "    \n",
    "    image = tf.image.decode_jpeg(image)    # Convert tensor into uint8 type tensor such as image matrix\n",
    "\n",
    "    input_image = tf.cast(image, tf.float32)  # Converting uint8 to float32\n",
    "    \n",
    "    input_image = (input_image / 255)\n",
    "    \n",
    "    return input_image\n",
    "\n",
    "input_data = tf.data.Dataset.list_files(os.path.join(path_inp + 'train/*.jpg'))\n",
    "input_data = input_data.map(norm_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "input_data = input_data.shuffle(200)\n",
    "input_data = input_data.batch(1)\n",
    "\n",
    "#if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "#        input_image = tf.transpose(input_image, [2, 0, 1])\n",
    "#        real_image = tf.transpose(real_image, [2, 0, 1])\n",
    "\n",
    "test_data = tf.data.Dataset.list_files(os.path.join(path_inp + 'test/*.jpg'))\n",
    "test_data = test_data.map(norm_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_data = test_data.batch(1)\n",
    "\n",
    "#if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "#        input_image = tf.transpose(input_image, [2, 0, 1])\n",
    "        \n",
    "        \n",
    "init = tf.random_normal_initializer(0.,0.02)\n",
    "\n",
    "def Generator_model():\n",
    "    \n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        I = tf.keras.layers.Input(shape=[3,256,256])\n",
    "    else:\n",
    "        I = tf.keras.layers.Input(shape=[256,256,3])\n",
    "    \n",
    "    #I = Input(shape=[256,256,3])\n",
    "    \n",
    "    C1 = Conv2D(64, (9,9), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(I)\n",
    "    L1 = LeakyReLU()(C1)\n",
    "    \n",
    "    C2 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L1)\n",
    "    B2 = BatchNormalization(axis=1)(C2)\n",
    "    L2 = LeakyReLU()(B2)\n",
    "    \n",
    "    C3 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L2)\n",
    "    B3 = BatchNormalization(axis=1)(C3)\n",
    "    L3 = LeakyReLU()(B3)\n",
    "    \n",
    "    A1 = Add()([L1,L3])\n",
    "    \n",
    "    C4 = Conv2D(64, (3,3),padding='same',  data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(A1)\n",
    "    B4 = BatchNormalization(axis=1)(C4)\n",
    "    L4 = LeakyReLU()(B4)\n",
    "    \n",
    "    C5 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L4)\n",
    "    B5 = BatchNormalization(axis=1)(C5)\n",
    "    L5 = LeakyReLU()(B5)\n",
    "    \n",
    "    U6 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L5)\n",
    "    B6 = BatchNormalization(axis=1)(U6)\n",
    "    L6 = LeakyReLU()(B6)\n",
    "    \n",
    "    A2 = Add()([L4, L6])\n",
    "    \n",
    "    U7 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(),kernel_initializer=init,use_bias=True)(A2)\n",
    "    B7 = BatchNormalization(axis=1)(U7)\n",
    "    L7 = LeakyReLU()(B7)\n",
    "    \n",
    "    U8 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(),kernel_initializer=init,use_bias=True)(L7)\n",
    "    B8 = BatchNormalization(axis=1)(U8)\n",
    "    L8 = LeakyReLU()(B8)\n",
    "    \n",
    "    U9 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L8)\n",
    "    B9 = BatchNormalization(axis=1)(U9)\n",
    "    L9 = LeakyReLU()(B9)\n",
    "    \n",
    "    A3 = Add()([L7, L9])\n",
    "    \n",
    "    U10 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A3)\n",
    "    B10 = BatchNormalization(axis=1)(U10)\n",
    "    L10 = LeakyReLU()(B10)\n",
    "    \n",
    "    U11 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L10)\n",
    "    B11 = BatchNormalization(axis=1)(U11)\n",
    "    L11 = LeakyReLU()(B11)\n",
    "    \n",
    "    U12 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L11)\n",
    "    B12 = BatchNormalization(axis=1)(U12)\n",
    "    L12 = LeakyReLU()(B12)\n",
    "    \n",
    "    A4 = Add()([L10, L12])\n",
    "    \n",
    "    U13 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A4)\n",
    "    B13 = BatchNormalization(axis=1)(U13)\n",
    "    L13 = LeakyReLU()(B13)\n",
    "    \n",
    "    U14 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L13)\n",
    "    B14 = BatchNormalization(axis=1)(U14)\n",
    "    L14 = LeakyReLU()(B14)\n",
    "    \n",
    "    U15 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L14)\n",
    "    B15 = BatchNormalization(axis=1)(U15)\n",
    "    L15 = LeakyReLU()(B15)\n",
    "    \n",
    "    A5 = Add()([L13, L15])\n",
    "    \n",
    "    U16 = Conv2D(256, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A5)\n",
    "    B16 = BatchNormalization(axis=1)(U16)\n",
    "    L16 = LeakyReLU()(B16)\n",
    "        \n",
    "    out = Conv2D(3, (1,1), kernel_initializer=init, data_format=tf.keras.backend.image_data_format(),  activation='tanh', use_bias=True)(L16)\n",
    "    \n",
    "    model = Model(inputs=I, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "generator = Generator_model()\n",
    "#generator.summary()\n",
    "\n",
    "def Gen_2():\n",
    "    \n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        I = tf.keras.layers.Input(shape=[3,256,256])\n",
    "    else:\n",
    "        I = tf.keras.layers.Input(shape=[256,256,3])\n",
    "        \n",
    "    #I = Input(shape=[256,256,3])\n",
    "    \n",
    "    C1 = Conv2D(64, (9,9), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(I)\n",
    "    L1 = LeakyReLU()(C1)\n",
    "    \n",
    "    C2 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L1)\n",
    "    B2 = BatchNormalization(axis=1)(C2)\n",
    "    L2 = LeakyReLU()(B2)\n",
    "    \n",
    "    C3 = Conv2D(64, (3,3), padding='same',  data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L2)\n",
    "    B3 = BatchNormalization(axis=1)(C3)\n",
    "    L3 = LeakyReLU()(B3)\n",
    "    \n",
    "    A1 = Add()([L1,L3])\n",
    "    \n",
    "    C4 = Conv2D(64, (3,3),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(A1)\n",
    "    B4 = BatchNormalization(axis=1)(C4)\n",
    "    L4 = LeakyReLU()(B4)\n",
    "    \n",
    "    C5 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L4)\n",
    "    B5 = BatchNormalization(axis=1)(C5)\n",
    "    L5 = LeakyReLU()(B5)\n",
    "    \n",
    "    U6 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L5)\n",
    "    B6 = BatchNormalization(axis=1)(U6)\n",
    "    L6 = LeakyReLU()(B6)\n",
    "    \n",
    "    A2 = Add()([L4, L6])\n",
    "    \n",
    "    U7 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A2)\n",
    "    B7 = BatchNormalization(axis=1)(U7)\n",
    "    L7 = LeakyReLU()(B7)\n",
    "    \n",
    "    U8 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(),kernel_initializer=init,use_bias=True)(L7)\n",
    "    B8 = BatchNormalization(axis=1)(U8)\n",
    "    L8 = LeakyReLU()(B8)\n",
    "    \n",
    "    U9 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L8)\n",
    "    B9 = BatchNormalization(axis=1)(U9)\n",
    "    L9 = LeakyReLU()(B9)\n",
    "    \n",
    "    A3 = Add()([L7, L9])\n",
    "    \n",
    "    U10 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A3)\n",
    "    B10 = BatchNormalization(axis=1)(U10)\n",
    "    L10 = LeakyReLU()(B10)\n",
    "    \n",
    "    U11 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L10)\n",
    "    B11 = BatchNormalization(axis=1)(U11)\n",
    "    L11 = LeakyReLU()(B11)\n",
    "    \n",
    "    U12 = Conv2D(64, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(L11)\n",
    "    B12 = BatchNormalization(axis=1)(U12)\n",
    "    L12 = LeakyReLU()(B12)\n",
    "    \n",
    "    A4 = Add()([L10, L12])\n",
    "    \n",
    "    U13 = Conv2D(128, (3,3), padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init,use_bias=True)(A4)\n",
    "    B13 = BatchNormalization(axis=1)(U13)\n",
    "    L13 = LeakyReLU()(B13)\n",
    "        \n",
    "    out = Conv2D(3, (1,1), kernel_initializer=init, data_format=tf.keras.backend.image_data_format(), activation='tanh', use_bias=True)(L13)\n",
    "    \n",
    "    model = Model(inputs=I, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "generator_2 = Gen_2()\n",
    "#generator_2.summary()\n",
    "\n",
    "def Discriminator_model():\n",
    "    \n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        inp = tf.keras.layers.Input(shape=[3, 256, 256], name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=[3, 256, 256], name='target_image')\n",
    "    else:\n",
    "        inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "    \n",
    "    \n",
    "    #inp = tf.keras.layers.Input(shape=[256,256,3], name='Input_Image')\n",
    "    #tar = tf.keras.layers.Input(shape=[256,256,3], name='Target_Image')\n",
    "    \n",
    "    x = concatenate([inp, tar])\n",
    "    \n",
    "    C1 = Conv2D(64, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(x)\n",
    "    L1 = LeakyReLU()(C1)\n",
    "    \n",
    "    C2 = Conv2D(128, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L1)\n",
    "    B2 = BatchNormalization(axis=1)(C2)\n",
    "    L2 = LeakyReLU()(B2)\n",
    "    \n",
    "    C3 = Conv2D(256, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L2)\n",
    "    B3 = BatchNormalization(axis=1)(C3)\n",
    "    L3 = LeakyReLU()(B3)\n",
    "    \n",
    "    C4 = Conv2D(512, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L3)\n",
    "    B4 = BatchNormalization(axis=1)(C4)\n",
    "    L4 = LeakyReLU()(B4)\n",
    "    \n",
    "    last = Conv2D(1, (1,1), kernel_initializer=init, data_format=tf.keras.backend.image_data_format(), activation='sigmoid')(L4) \n",
    "\n",
    "    model = Model(inputs=[inp, tar], outputs=last)\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = Discriminator_model()\n",
    "#discriminator.summary()\n",
    "\n",
    "def Disc_2():\n",
    "    \n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        inp = tf.keras.layers.Input(shape=[3, 256, 256], name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=[3, 256, 256], name='target_image')\n",
    "    else:\n",
    "        inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "    \n",
    "    #inp = tf.keras.layers.Input(shape=[256,256,3], name='Input_Image')\n",
    "    #tar = tf.keras.layers.Input(shape=[256,256,3], name='Target_Image')\n",
    "    \n",
    "    x = concatenate([inp, tar])\n",
    "    \n",
    "    C1 = Conv2D(64, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(x)\n",
    "    L1 = BatchNormalization(axis=1)(C1)\n",
    "    \n",
    "    C2 = Conv2D(128, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L1)\n",
    "    B2 = BatchNormalization(axis=1)(C2)\n",
    "    L2 = LeakyReLU()(B2)\n",
    "    \n",
    "    C3 = Conv2D(256, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L2)\n",
    "    B3 = BatchNormalization(axis=1)(C3)\n",
    "    L3 = LeakyReLU()(B3)\n",
    "    \n",
    "    C4 = Conv2D(512, (4,4),padding='same', data_format=tf.keras.backend.image_data_format(), kernel_initializer=init, use_bias=True)(L3)\n",
    "    B4 = BatchNormalization(axis=1)(C4)\n",
    "    L4 = LeakyReLU()(B4)\n",
    "    \n",
    "    last = Conv2D(1, (1,1), kernel_initializer=init, data_format=tf.keras.backend.image_data_format(), activation='sigmoid')(L4) \n",
    "\n",
    "    model = Model(inputs=[inp, tar], outputs=last)\n",
    "    \n",
    "    return model\n",
    "\n",
    "disc_2 = Disc_2()\n",
    "#disc_2.summary()\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    \n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    l1_loss = tf.reduce_mean(tf.keras.losses.MAE(target, gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss\n",
    "\n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    \n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss\n",
    "\n",
    "def generator_2_loss(disc_generated_output_2, gen_2_output, target):\n",
    "    \n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output_2), disc_generated_output_2)\n",
    "    \n",
    "    #l1_loss = tf.reduce_mean(tf.keras.losses.MAE(target, gen_2_output))\n",
    "    \n",
    "    #ssim = -tf.reduce_mean(tf.image.ssim(target,gen_2_output, max_val=2.0))\n",
    "    \n",
    "    l2_loss = tf.reduce_mean(tf.keras.losses.MSE(target, gen_2_output)) \n",
    "    \n",
    "    total_gen_loss = gan_loss + 50*l2_loss\n",
    "\n",
    "    return total_gen_loss\n",
    "\n",
    "\n",
    "def disc_2_loss(disc_real_output_2, disc_generated_output_2):\n",
    "    \n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output_2), disc_real_output_2)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output_2), disc_generated_output_2)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_2_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "disc_2_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"TF\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator_2_optimizer=generator_2_optimizer,disc_2_optimizer=disc_2_optimizer,\n",
    "                                 generator=generator,discriminator=discriminator,generator_2= generator_2,disc_2=disc_2)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as gen_2_tape, tf.GradientTape() as disc_2_tape:\n",
    "\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "\n",
    "\n",
    "        gen_total_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "\n",
    "        gen_2_output = generator_2(gen_output, training=True) \n",
    "\n",
    "        disc_real_output_2 = disc_2([gen_output, target], training=True)\n",
    "\n",
    "        disc_generated_output_2 = disc_2([gen_output, gen_2_output ], training=True)\n",
    "\n",
    "\n",
    "        gen_total_loss_2 = generator_2_loss(disc_generated_output_2, gen_2_output, target)\n",
    "\n",
    "        disc_loss_2 = disc_2_loss(disc_real_output_2, disc_generated_output_2)\n",
    "\n",
    "\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,generator.trainable_variables)\n",
    "\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "        \n",
    "    gen_2_gradients = gen_2_tape.gradient(gen_total_loss_2, generator_2.trainable_variables)\n",
    "\n",
    "    disc_2_gradients = disc_2_tape.gradient(disc_loss_2, disc_2.trainable_variables)\n",
    "\n",
    "    generator_2_optimizer.apply_gradients(zip(gen_2_gradients, generator_2.trainable_variables))\n",
    "\n",
    "    disc_2_optimizer.apply_gradients(zip(disc_2_gradients, disc_2.trainable_variables)) \n",
    "\n",
    "    return gen_total_loss , disc_loss, disc_loss_2, gen_total_loss_2\n",
    "\n",
    "def generate_images(test_input):\n",
    "    \n",
    "    prediction = generator(test_input)\n",
    "    \n",
    "    return  prediction \n",
    "\n",
    "def generate_images_2(test_input):\n",
    "    \n",
    "    output = generator(test_input)\n",
    "    \n",
    "    prediction = generator_2(output)\n",
    "    \n",
    "    return  prediction \n",
    "\n",
    "def train(input_data, epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()    \n",
    "        \n",
    "        for n, (input1, target) in input_data.enumerate():\n",
    "          \n",
    "           #if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "            D1, G1, D2, G2 = train_step(input1, target, epoch)\n",
    "\n",
    "           \n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        \n",
    "            \n",
    "        print(\"Epoch: \", epoch)\n",
    "        print('Disc loss 1:', D1)\n",
    "        print('Gen loss 1:', G1)\n",
    "        print('Disc loss 2:', D2)\n",
    "        print('Gen loss 2:', G2)\n",
    "        print()\n",
    "            \n",
    "        if not os.path.isdir(res_dire_1 + '%04d' % epoch):\n",
    "                os.makedirs(res_dire_1 + '%04d' % epoch)   \n",
    "        k = 1\n",
    "        for inp in test_data.take(50):\n",
    "            img = generate_images(inp)\n",
    "            im = np.squeeze(img)\n",
    "            im1 = im*0.5 + 0.05 # To make pixels betn 0 to 1\n",
    "            out = np.minimum(np.maximum(im1, 0), 1)\n",
    "            my = Image.fromarray((im1*255).astype(np.uint8), mode='RGB')\n",
    "            my.save(res_dire_1 + '%04d/%d.jpg' % (epoch,k))\n",
    "            k = k + 1\n",
    "        \n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n",
    "            \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "\n",
    "EPOCHS = 40\n",
    "E = 20\n",
    "train(input_data, EPOCHS)\n",
    "\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "k = 1\n",
    "for inp in test_data.take(50):\n",
    "    img = generate_images_2(inp)\n",
    "    im = np.squeeze(img)\n",
    "    im1 = im*0.5 + 0.1 # To make pixels betn 0 to 1\n",
    "    out = np.minimum(np.maximum(im1, 0), 1)\n",
    "    my = Image.fromarray((im1*255).astype(np.uint8), mode='RGB')\n",
    "    my.save(res_dire_2 + '%d.jpg' % k)\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
